{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compass using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# For preprocessing the data\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "# Standardizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# To split the dataset into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# To calculate the accuracy score of the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "#\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "#\n",
    "import collections\n",
    "#\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "path = 'C:\\\\Users\\\\rivas\\\\OneDrive\\\\Documents\\\\JMR\\\\Education\\\\Springboard\\\\Projects\\\\Capstone2\\\\'\n",
    "int_fnm = path + 'data\\\\compass\\\\compas-scores-raw.csv'\n",
    "df = pd.read_csv(int_fnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "African-American    27069\n",
      "Caucasian           21783\n",
      "Hispanic             8742\n",
      "Other                2592\n",
      "Asian                 324\n",
      "Native American       219\n",
      "Arabic                 75\n",
      "Oriental               39\n",
      "Name: Ethnic_Code_Text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Review data and Manipulate Data\n",
    "\n",
    "# update 'Ethnic_Code_Text' to have conistent values for African Americans\n",
    "df.loc[df['Ethnic_Code_Text'] == 'African-Am', 'Ethnic_Code_Text'] = 'African-American'\n",
    "print(pd.value_counts(df['Ethnic_Code_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "1     18465\n",
      "2      9192\n",
      "3      8492\n",
      "4      5338\n",
      "5      4831\n",
      "6      4319\n",
      "7      3338\n",
      "8      2799\n",
      "9      2386\n",
      "10     1638\n",
      "Name: DecileScore, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# DecileScore should be between 1 & 10, delete otherwise\n",
    "df.DecileScore.unique()\n",
    "print((df['DecileScore'] < 1).sum())\n",
    "# remove DecileScore < 1\n",
    "df = df[df.DecileScore >= 1]\n",
    "(df['DecileScore'] < 1).sum()\n",
    "print(pd.value_counts(df['DecileScore']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60798 60798\n",
      "Index(['Person_ID', 'AssessmentID', 'Case_ID', 'Agency_Text', 'LastName',\n",
      "       'FirstName', 'MiddleName', 'Sex_Code_Text', 'Ethnic_Code_Text',\n",
      "       'DateOfBirth', 'ScaleSet_ID', 'ScaleSet', 'AssessmentReason',\n",
      "       'Language', 'LegalStatus', 'CustodyStatus', 'MaritalStatus',\n",
      "       'Screening_Date', 'RecSupervisionLevel', 'RecSupervisionLevelText',\n",
      "       'Scale_ID', 'DisplayText', 'RawScore', 'DecileScore', 'ScoreText',\n",
      "       'AssessmentType', 'IsCompleted', 'IsDeleted', 'Age'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Add column 'Age' from DateofBirth\n",
    "agelist = []\n",
    "currdate = date.today()\n",
    "for dte in df['DateOfBirth']:\n",
    "    brthdte = datetime.strptime(dte, '%m/%d/%y')\n",
    "    mnthday = (currdate.month, currdate.day) < (brthdte.month, brthdte.day)\n",
    "    if currdate.year > brthdte.year:\n",
    "        agelist.append(currdate.year - brthdte.year - (mnthday))\n",
    "    else:\n",
    "        agelist.append(-1)\n",
    "    \n",
    "\n",
    "print(len(agelist), len(df))\n",
    "df['Age'] = agelist\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup bad Ages\n",
    "# remove Ages < 1\n",
    "(df['Age'] < 1).sum()\n",
    "\n",
    "df = df[df.Age >= 1]\n",
    "(df['Age'] < 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appear: (16016, 29)  Violence:  (16010, 29)  Recidivism: (15990, 29)\n"
     ]
    }
   ],
   "source": [
    "# Slice by 'DisplayText' for Risk\n",
    "RiskAppear = df.loc[df['DisplayText'] == 'Risk of Failure to Appear']\n",
    "RiskViolence = df.loc[df['DisplayText'] == 'Risk of Violence']\n",
    "RiskRecidivism = df.loc[df['DisplayText'] == 'Risk of Recidivism']\n",
    "print('Appear:', RiskAppear.shape, ' Violence: ', RiskViolence.shape,  ' Recidivism:',RiskRecidivism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define prepare_data_for_ml_model_1:\n",
    "def prepare_data_for_ml_model_1(dfx, target_loc):\n",
    "    # Create new Dataset of selected columns to get prepare TEST and Training data for  ML model \n",
    "     \n",
    "    \"\"\"\n",
    "    Columns\n",
    "    0 - 4  : 'Person_ID','AssessmentID','Case_ID','Agency_Text', 'LastName',\n",
    "    5 - 9  : 'FirstName', 'MiddleName', 'Sex_Code_Text', 'Ethnic_Code_Text','DateOfBirth',\n",
    "    10 - 14: 'ScaleSet_ID', 'ScaleSet', 'AssessmentReason','Language', 'LegalStatus',\n",
    "    15 - 19: 'CustodyStatus', 'MaritalStatus','Screening_Date', 'RecSupervisionLevel', 'RecSupervisionLevelText',\n",
    "    20 - 24: 'Scale_ID', 'DisplayText', 'RawScore', 'DecileScore', 'ScoreText',\n",
    "    25 - 28: 'AssessmentType', 'IsCompleted', 'IsDeleted','Age'\n",
    "    \"\"\"\n",
    "\n",
    "    x_df = dfx.iloc[:, [7,8,14,15,16,19]] #features\n",
    "    tmp_age = dfx.iloc[:,28].as_matrix() #age feature, convert numpy array\n",
    "    x_age = tmp_age.reshape(tmp_age.size,1)\n",
    "    \n",
    "\n",
    "    y = dfx.iloc[:,target_loc].as_matrix() #target convert numpy array\n",
    "\n",
    "\n",
    "    #  lable encoder. It encodes the data into integers\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    Sex_Code_Text_cat = le.fit_transform(x_df.Sex_Code_Text)\n",
    "    Ethnic_Code_Text_cat = le.fit_transform(x_df.Ethnic_Code_Text)\n",
    "    LegalStatus_cat = le.fit_transform(x_df.LegalStatus)\n",
    "    CustodyStatus_cat = le.fit_transform(x_df.CustodyStatus)\n",
    "    MaritalStatus_cat = le.fit_transform(x_df.MaritalStatus)\n",
    "    RecSupervisionLevelText_cat = le.fit_transform(x_df.RecSupervisionLevelText)\n",
    "\n",
    "    Sex_Code_Text_cat = Sex_Code_Text_cat.reshape(len(Sex_Code_Text_cat),1)\n",
    "    Ethnic_Code_Text_cat = Ethnic_Code_Text_cat.reshape(len(Ethnic_Code_Text_cat),1)\n",
    "    LegalStatus_cat = LegalStatus_cat.reshape(len(LegalStatus_cat),1)\n",
    "    CustodyStatus_cat = CustodyStatus_cat.reshape(len(CustodyStatus_cat),1)\n",
    "    MaritalStatus_cat = MaritalStatus_cat.reshape(len(MaritalStatus_cat),1)\n",
    "    RecSupervisionLevelText_cat = RecSupervisionLevelText_cat.reshape(len(RecSupervisionLevelText_cat),1)\n",
    "\n",
    "#  One-Hot encoder. It encodes the data into binary format\n",
    "    onehote = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    Sex_Code_Text_oh = onehote.fit_transform(Sex_Code_Text_cat)\n",
    "    Ethnic_Code_Text_oh = onehote.fit_transform(Ethnic_Code_Text_cat)\n",
    "    LegalStatus_oh = onehote.fit_transform(LegalStatus_cat)\n",
    "    CustodyStatus_oh = onehote.fit_transform(CustodyStatus_cat)\n",
    "    MaritalStatus_oh = onehote.fit_transform(MaritalStatus_cat)\n",
    "    RecSupervisionLevelText_oh = onehote.fit_transform(RecSupervisionLevelText_cat)\n",
    "\n",
    "# Build out feature dataset as numpy array, since One-Hot encoder creates numpy array\n",
    "    X_feature =  Sex_Code_Text_oh\n",
    "    X_feature = np.concatenate((X_feature,Ethnic_Code_Text_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,LegalStatus_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,CustodyStatus_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,MaritalStatus_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,RecSupervisionLevelText_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,x_age), axis=1)\n",
    "\n",
    "# Split data train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.2)\n",
    "    print('Length for X_train:', len(X_train), ' X_test:',len(X_test), ' y_train:',len(y_train) ,' y_test:',len(y_test))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define ml_model_1:\n",
    "# takes in model Instantiate model (model)\n",
    "# fits, predicts, and evaluates (prints results)\n",
    "def ml_model_1(model, modelnm, dfnm, X_train, X_test, y_train, y_test, target):\n",
    "    print('Running ', modelnm, ' model for :', dfnm, ' using target: ', target)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Evaluate\n",
    "    print(modelnm,' score: ',model.score(X_test, y_test))\n",
    "    print(' ')\n",
    "    print('first 10 predicted values: ',y_pred[0:10])\n",
    "    print('first 10 values of target: ')\n",
    "    print(y_test[0:10])\n",
    "    print(' ')\n",
    "\n",
    "    print('mean of predicted values: ',np.mean(y_pred), ' STD of predicted values : ', np.std(y_pred) )\n",
    "    print('mean of Target  values: ',np.mean(y_pred), ' STD of predicted  values : ', np.std(y_pred) )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataReader(object):\n",
    "\n",
    "    def __init__(self,*arrays,batch_size=1):\n",
    "        self.arrays = arrays\n",
    "        self.__check_equal_shape()\n",
    "        self.num_examples = self.arrays[0].shape[0]\n",
    "        self.batch_number = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = int(np.ceil(self.num_examples / batch_size))\n",
    "\n",
    "    def __check_equal_shape(self):\n",
    "        if any(self.arrays[0].shape[0] != arr.shape[0] for arr in self.arrays[1:]):\n",
    "            raise ValueError(\"all arrays must be equal along first dimension\")\n",
    "\n",
    "    def next_batch(self):\n",
    "        low_ix = self.batch_number*self.batch_size\n",
    "        up_ix = (self.batch_number + 1)*self.batch_size\n",
    "        if up_ix >= self.num_examples:\n",
    "            up_ix = self.num_examples\n",
    "            self.batch_number = 0 # reset batch_number to zero\n",
    "        else:\n",
    "            self.batch_number = self.batch_number + 1\n",
    "\n",
    "        return [arr[low_ix:up_ix,:] for arr in self.arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Tensorflow  Implementation #page 422\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for X_train: 12792  X_test: 3198  y_train: 12792  y_test: 3198\n"
     ]
    }
   ],
   "source": [
    "# RiskRecidivism dataset target RawScore (22)\n",
    "# X_train.shape (12792, 35)\n",
    "X_train, X_test, y_train, y_test = prepare_data_for_ml_model_1(RiskRecidivism,22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12792, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rivas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X_train)\n",
    "X = tf.constant(X_scaler, dtype=tf.float32, name=\"X\")\n",
    "\n",
    "n_inputs = 35 # 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  \n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer() # He initialization\n",
    "#Equivalent to:\n",
    "#he_init = lambda shape, dtype=tf.float32: tf.truncated_normal(shape, 0., stddev=np.sqrt(2/shape[0]))\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "my_dense_layer = partial(tf.layers.dense,\n",
    "                         activation=tf.nn.elu,\n",
    "                         kernel_initializer=he_init,\n",
    "                         kernel_regularizer=l2_regularizer)\n",
    "\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3 = my_dense_layer(hidden2, n_hidden3)\n",
    "outputs = my_dense_layer(hidden3, n_outputs, activation=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12792, 35) (12792,) (3198, 35) (3198,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatianate\n",
    "data_reader = dataReader(X_train,y_train, batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "print(data_reader.num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0%"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "next_batch() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-414d4ae5e8fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r{}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: next_batch() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        #n_batches = mnist.train.num_examples // batch_size\n",
    "        n_batches = data_reader.num_batches\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()                                          \n",
    "            X_batch, y_batch = data_reader.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})   \n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)           \n",
    "        saver.save(sess, \"./my_model_all_layers.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
