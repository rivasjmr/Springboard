{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compass using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# For preprocessing the data\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "# Standardizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# To split the dataset into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# To calculate the accuracy score of the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "#\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "#\n",
    "import collections\n",
    "#\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "path = 'C:\\\\Users\\\\rivas\\\\OneDrive\\\\Documents\\\\JMR\\\\Education\\\\Springboard\\\\Projects\\\\Capstone2\\\\'\n",
    "int_fnm = path + 'data\\\\compass\\\\compas-scores-raw.csv'\n",
    "df = pd.read_csv(int_fnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "African-American    27069\n",
      "Caucasian           21783\n",
      "Hispanic             8742\n",
      "Other                2592\n",
      "Asian                 324\n",
      "Native American       219\n",
      "Arabic                 75\n",
      "Oriental               39\n",
      "Name: Ethnic_Code_Text, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Review data and Manipulate Data\n",
    "\n",
    "# update 'Ethnic_Code_Text' to have conistent values for African Americans\n",
    "df.loc[df['Ethnic_Code_Text'] == 'African-Am', 'Ethnic_Code_Text'] = 'African-American'\n",
    "print(pd.value_counts(df['Ethnic_Code_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "1     18465\n",
      "2      9192\n",
      "3      8492\n",
      "4      5338\n",
      "5      4831\n",
      "6      4319\n",
      "7      3338\n",
      "8      2799\n",
      "9      2386\n",
      "10     1638\n",
      "Name: DecileScore, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# DecileScore should be between 1 & 10, delete otherwise\n",
    "df.DecileScore.unique()\n",
    "print((df['DecileScore'] < 1).sum())\n",
    "# remove DecileScore < 1\n",
    "df = df[df.DecileScore >= 1]\n",
    "(df['DecileScore'] < 1).sum()\n",
    "print(pd.value_counts(df['DecileScore']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60798 60798\n",
      "Index(['Person_ID', 'AssessmentID', 'Case_ID', 'Agency_Text', 'LastName',\n",
      "       'FirstName', 'MiddleName', 'Sex_Code_Text', 'Ethnic_Code_Text',\n",
      "       'DateOfBirth', 'ScaleSet_ID', 'ScaleSet', 'AssessmentReason',\n",
      "       'Language', 'LegalStatus', 'CustodyStatus', 'MaritalStatus',\n",
      "       'Screening_Date', 'RecSupervisionLevel', 'RecSupervisionLevelText',\n",
      "       'Scale_ID', 'DisplayText', 'RawScore', 'DecileScore', 'ScoreText',\n",
      "       'AssessmentType', 'IsCompleted', 'IsDeleted', 'Age'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Add column 'Age' from DateofBirth\n",
    "agelist = []\n",
    "currdate = date.today()\n",
    "for dte in df['DateOfBirth']:\n",
    "    brthdte = datetime.strptime(dte, '%m/%d/%y')\n",
    "    mnthday = (currdate.month, currdate.day) < (brthdte.month, brthdte.day)\n",
    "    if currdate.year > brthdte.year:\n",
    "        agelist.append(currdate.year - brthdte.year - (mnthday))\n",
    "    else:\n",
    "        agelist.append(-1)\n",
    "    \n",
    "\n",
    "print(len(agelist), len(df))\n",
    "df['Age'] = agelist\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup bad Ages\n",
    "# remove Ages < 1\n",
    "(df['Age'] < 1).sum()\n",
    "\n",
    "df = df[df.Age >= 1]\n",
    "(df['Age'] < 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appear: (16016, 29)  Violence:  (16010, 29)  Recidivism: (15990, 29)\n"
     ]
    }
   ],
   "source": [
    "# Slice by 'DisplayText' for Risk\n",
    "RiskAppear = df.loc[df['DisplayText'] == 'Risk of Failure to Appear']\n",
    "RiskViolence = df.loc[df['DisplayText'] == 'Risk of Violence']\n",
    "RiskRecidivism = df.loc[df['DisplayText'] == 'Risk of Recidivism']\n",
    "print('Appear:', RiskAppear.shape, ' Violence: ', RiskViolence.shape,  ' Recidivism:',RiskRecidivism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define prepare_data_for_ml_model_1:\n",
    "def prepare_data_for_ml_model_1(dfx, target_loc):\n",
    "    # Create new Dataset of selected columns to get prepare TEST and Training data for  ML model \n",
    "     \n",
    "    \"\"\"\n",
    "    Columns\n",
    "    0 - 4  : 'Person_ID','AssessmentID','Case_ID','Agency_Text', 'LastName',\n",
    "    5 - 9  : 'FirstName', 'MiddleName', 'Sex_Code_Text', 'Ethnic_Code_Text','DateOfBirth',\n",
    "    10 - 14: 'ScaleSet_ID', 'ScaleSet', 'AssessmentReason','Language', 'LegalStatus',\n",
    "    15 - 19: 'CustodyStatus', 'MaritalStatus','Screening_Date', 'RecSupervisionLevel', 'RecSupervisionLevelText',\n",
    "    20 - 24: 'Scale_ID', 'DisplayText', 'RawScore', 'DecileScore', 'ScoreText',\n",
    "    25 - 28: 'AssessmentType', 'IsCompleted', 'IsDeleted','Age'\n",
    "    \"\"\"\n",
    "\n",
    "    x_df = dfx.iloc[:, [7,8,14,15,16,19]] #features\n",
    "    tmp_age = dfx.iloc[:,28].as_matrix() #age feature, convert numpy array\n",
    "    x_age = tmp_age.reshape(tmp_age.size,1)\n",
    "    \n",
    "\n",
    "    y = dfx.iloc[:,target_loc].as_matrix() #target convert numpy array\n",
    "\n",
    "\n",
    "    #  lable encoder. It encodes the data into integers\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    Sex_Code_Text_cat = le.fit_transform(x_df.Sex_Code_Text)\n",
    "    Ethnic_Code_Text_cat = le.fit_transform(x_df.Ethnic_Code_Text)\n",
    "    LegalStatus_cat = le.fit_transform(x_df.LegalStatus)\n",
    "    CustodyStatus_cat = le.fit_transform(x_df.CustodyStatus)\n",
    "    MaritalStatus_cat = le.fit_transform(x_df.MaritalStatus)\n",
    "    RecSupervisionLevelText_cat = le.fit_transform(x_df.RecSupervisionLevelText)\n",
    "\n",
    "    Sex_Code_Text_cat = Sex_Code_Text_cat.reshape(len(Sex_Code_Text_cat),1)\n",
    "    Ethnic_Code_Text_cat = Ethnic_Code_Text_cat.reshape(len(Ethnic_Code_Text_cat),1)\n",
    "    LegalStatus_cat = LegalStatus_cat.reshape(len(LegalStatus_cat),1)\n",
    "    CustodyStatus_cat = CustodyStatus_cat.reshape(len(CustodyStatus_cat),1)\n",
    "    MaritalStatus_cat = MaritalStatus_cat.reshape(len(MaritalStatus_cat),1)\n",
    "    RecSupervisionLevelText_cat = RecSupervisionLevelText_cat.reshape(len(RecSupervisionLevelText_cat),1)\n",
    "\n",
    "#  One-Hot encoder. It encodes the data into binary format\n",
    "    onehote = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    Sex_Code_Text_oh = onehote.fit_transform(Sex_Code_Text_cat)\n",
    "    Ethnic_Code_Text_oh = onehote.fit_transform(Ethnic_Code_Text_cat)\n",
    "    LegalStatus_oh = onehote.fit_transform(LegalStatus_cat)\n",
    "    CustodyStatus_oh = onehote.fit_transform(CustodyStatus_cat)\n",
    "    MaritalStatus_oh = onehote.fit_transform(MaritalStatus_cat)\n",
    "    RecSupervisionLevelText_oh = onehote.fit_transform(RecSupervisionLevelText_cat)\n",
    "\n",
    "# Build out feature dataset as numpy array, since One-Hot encoder creates numpy array\n",
    "    X_feature =  Sex_Code_Text_oh\n",
    "    X_feature = np.concatenate((X_feature,Ethnic_Code_Text_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,LegalStatus_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,CustodyStatus_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,MaritalStatus_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,RecSupervisionLevelText_oh), axis=1)\n",
    "    X_feature = np.concatenate((X_feature,x_age), axis=1)\n",
    "\n",
    "# Split data train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.2)\n",
    "    print('Length for X_train:', len(X_train), ' X_test:',len(X_test), ' y_train:',len(y_train) ,' y_test:',len(y_test))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define ml_model_1:\n",
    "# takes in model Instantiate model (model)\n",
    "# fits, predicts, and evaluates (prints results)\n",
    "def ml_model_1(model, modelnm, dfnm, X_train, X_test, y_train, y_test, target):\n",
    "    print('Running ', modelnm, ' model for :', dfnm, ' using target: ', target)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Evaluate\n",
    "    print(modelnm,' score: ',model.score(X_test, y_test))\n",
    "    print(' ')\n",
    "    print('first 10 predicted values: ',y_pred[0:10])\n",
    "    print('first 10 values of target: ')\n",
    "    print(y_test[0:10])\n",
    "    print(' ')\n",
    "\n",
    "    print('mean of predicted values: ',np.mean(y_pred), ' STD of predicted values : ', np.std(y_pred) )\n",
    "    print('mean of Target  values: ',np.mean(y_pred), ' STD of predicted  values : ', np.std(y_pred) )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataReader(object):\n",
    "\n",
    "    def __init__(self,*arrays,batch_size=1):\n",
    "        self.arrays = arrays\n",
    "        self.__check_equal_shape()\n",
    "        self.num_examples = self.arrays[0].shape[0]\n",
    "        self.batch_number = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = int(np.ceil(self.num_examples / batch_size))\n",
    "\n",
    "    def __check_equal_shape(self):\n",
    "        if any(self.arrays[0].shape[0] != arr.shape[0] for arr in self.arrays[1:]):\n",
    "            raise ValueError(\"all arrays must be equal along first dimension\")\n",
    "\n",
    "    def next_batch(self):\n",
    "        low_ix = self.batch_number*self.batch_size\n",
    "        up_ix = (self.batch_number + 1)*self.batch_size\n",
    "        if up_ix >= self.num_examples:\n",
    "            up_ix = self.num_examples\n",
    "            self.batch_number = 0 # reset batch_number to zero\n",
    "        else:\n",
    "            self.batch_number = self.batch_number + 1\n",
    "\n",
    "        return [arr[low_ix:up_ix,:] for arr in self.arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Tensorflow  Implementation #page 422\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for X_train: 12792  X_test: 3198  y_train: 12792  y_test: 3198\n"
     ]
    }
   ],
   "source": [
    "# RiskRecidivism dataset target RawScore (22)\n",
    "# X_train.shape (12792, 35)\n",
    "X_train, X_test, y_train, y_test = prepare_data_for_ml_model_1(RiskRecidivism,22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12792, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X_train)\n",
    "# X = tf.constant(X_scaler, dtype=tf.float32, name=\"X\")\n",
    "X = tf.placeholder(tf.float32, shape=[None, 35], name=\"X\")\n",
    "\n",
    "\n",
    "n_inputs = 35 # 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  \n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer() # He initialization\n",
    "#Equivalent to:\n",
    "#he_init = lambda shape, dtype=tf.float32: tf.truncated_normal(shape, 0., stddev=np.sqrt(2/shape[0]))\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "my_dense_layer = partial(tf.layers.dense,\n",
    "                         activation=tf.nn.elu,\n",
    "                         kernel_initializer=he_init,\n",
    "                         kernel_regularizer=l2_regularizer)\n",
    "\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3 = my_dense_layer(hidden2, n_hidden3)\n",
    "outputs = my_dense_layer(hidden3, n_outputs, activation=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12792, 35) (12792,) (3198, 35) (3198,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatianate\n",
    "data_reader = dataReader(X_scaler,y_train[:,np.newaxis]\n",
    ", batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "print(data_reader.num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.08208996\n",
      "1 Train MSE: 0.036423102\n",
      "2 Train MSE: 0.046493594\n",
      "3 Train MSE: 0.06692738\n",
      "48% Train MSE: 0.0759481\n",
      "5 Train MSE: 0.054686897\n",
      "68% Train MSE: 95.27553\n",
      "7 Train MSE: 2.0933988\n",
      "8 Train MSE: 0.7479765\n",
      "9 Train MSE: 0.6214325\n",
      "10 Train MSE: 0.6019776\n",
      "11% Train MSE: 0.57499427\n",
      "12 Train MSE: 0.55758125\n",
      "13 Train MSE: 0.54443765\n",
      "14% Train MSE: 0.53859425\n",
      "15 Train MSE: 0.535984\n",
      "16 Train MSE: 0.5337672\n",
      "17 Train MSE: 0.531228\n",
      "18 Train MSE: 0.5286888\n",
      "19 Train MSE: 0.5263756\n",
      "20 Train MSE: 0.5233738\n",
      "21 Train MSE: 0.52035826\n",
      "22 Train MSE: 0.51767373\n",
      "23% Train MSE: 0.5147729\n",
      "24 Train MSE: 0.5119624\n",
      "25 Train MSE: 0.50870806\n",
      "26% Train MSE: 0.50545067\n",
      "27 Train MSE: 0.5022831\n",
      "28 Train MSE: 0.4995453\n",
      "29 Train MSE: 0.49735722\n",
      "30 Train MSE: 0.4950929\n",
      "31% Train MSE: 0.48991403\n",
      "32% Train MSE: 0.48815116\n",
      "33 Train MSE: 0.4825832\n",
      "34 Train MSE: 0.48123202\n",
      "35 Train MSE: 0.47364748\n",
      "36% Train MSE: 0.47477338\n",
      "37 Train MSE: 0.47356114\n",
      "38 Train MSE: 0.46711612\n",
      "39 Train MSE: 0.46917555\n",
      "40 Train MSE: 0.46431595\n",
      "41 Train MSE: 0.4617828\n",
      "42 Train MSE: 0.46074212\n",
      "43 Train MSE: 0.4660709\n",
      "44% Train MSE: 0.4526698\n",
      "45 Train MSE: 0.44992083\n",
      "46 Train MSE: 0.44768876\n",
      "47 Train MSE: 0.44623348\n",
      "48 Train MSE: 0.44466543\n",
      "49 Train MSE: 0.44301808\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        #n_batches = mnist.train.num_examples // batch_size\n",
    "        n_batches = data_reader.num_batches\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()                                          \n",
    "            X_batch, y_batch = data_reader.next_batch()\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})   \n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)           \n",
    "        saver.save(sess, \"./my_model_all_layers.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12792, 35)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\rivas\\\\Compass'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "n_inputs = 35 # 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  \n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "n_inputs = 35\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 20  # codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X_train)\n",
    "# X = tf.constant(X_scaler, dtype=tf.float32, name=\"X\")\n",
    "X = tf.placeholder(tf.float32, shape=[None, 35], name=\"X\")\n",
    "\"\"\"\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense,\n",
    "    activation=tf.nn.elu,\n",
    "    kernel_initializer=initializer)\n",
    "\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X_train)\n",
    "X = tf.placeholder(tf.float32, shape=[None, 35], name=\"X\")\n",
    "#X = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "#\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "hidden3_sigma = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)\n",
    "hidden3 = hidden3_mean + hidden3_sigma * noise\n",
    "hidden4 = my_dense_layer(hidden3, n_hidden4)\n",
    "hidden5 = my_dense_layer(hidden4, n_hidden5)\n",
    "logits = my_dense_layer(hidden5, n_outputs, activation=None)\n",
    "outputs = tf.sigmoid(logits)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\n",
    "reconstruction_loss = tf.reduce_sum(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instatianate\n",
    "data_reader = dataReader(X_scaler,y_train[:,np.newaxis]\n",
    ", batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-10 # smoothing term to avoid computing log(0) which is NaN\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.square(hidden3_sigma) + tf.square(hidden3_mean)\n",
    "    - 1 - tf.log(eps + tf.square(hidden3_sigma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = reconstruction_loss + latent_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08% Train total loss: -107718640.0 \tReconstruction loss: -174839540.0 \tLatent loss: 67120900.0\n",
      "1 Train total loss: -38463558000.0 \tReconstruction loss: -67983983000.0 \tLatent loss: 29520425000.0\n",
      "2 Train total loss: -1007263800000.0 \tReconstruction loss: -1813253200000.0 \tLatent loss: 805989400000.0\n",
      "3 Train total loss: -8951976000000.0 \tReconstruction loss: -16372679000000.0 \tLatent loss: 7420702000000.0\n",
      "4 Train total loss: -44866250000000.0 \tReconstruction loss: -82508234000000.0 \tLatent loss: 37641983000000.0\n",
      "5 Train total loss: -158693270000000.0 \tReconstruction loss: -293730100000000.0 \tLatent loss: 135036830000000.0\n",
      "6 Train total loss: -447246420000000.0 \tReconstruction loss: -832292050000000.0 \tLatent loss: 385045630000000.0\n",
      "7 Train total loss: -1069544300000000.0 \tReconstruction loss: -1997439500000000.0 \tLatent loss: 927895200000000.0\n",
      "8 Train total loss: -2262213800000000.0 \tReconstruction loss: -4239711200000000.0 \tLatent loss: 1977497400000000.0\n",
      "9 Train total loss: -4368903300000000.0 \tReconstruction loss: -8206448000000000.0 \tLatent loss: 3837545000000000.0\n",
      "10% Train total loss: -7820494400000000.0 \tReconstruction loss: -1.4717499e+16 \tLatent loss: 6897004500000000.0\n",
      "11 Train total loss: -1.3183078e+16 \tReconstruction loss: -2.4850576e+16 \tLatent loss: 1.1667498e+16\n",
      "12 Train total loss: -2.111452e+16 \tReconstruction loss: -3.9902407e+16 \tLatent loss: 1.8787886e+16\n",
      "13 Train total loss: -3.2521662e+16 \tReconstruction loss: -6.1534937e+16 \tLatent loss: 2.9013275e+16\n",
      "14 Train total loss: -4.8287115e+16 \tReconstruction loss: -9.1481876e+16 \tLatent loss: 4.319476e+16\n",
      "15 Train total loss: -6.953339e+16 \tReconstruction loss: -1.3186854e+17 \tLatent loss: 6.2335154e+16\n",
      "16 Train total loss: -9.7812795e+16 \tReconstruction loss: -1.8549744e+17 \tLatent loss: 8.768464e+16\n",
      "17 Train total loss: -1.3426168e+17 \tReconstruction loss: -2.548826e+17 \tLatent loss: 1.2062092e+17\n",
      "18 Train total loss: -1.8061578e+17 \tReconstruction loss: -3.4295915e+17 \tLatent loss: 1.6234337e+17\n",
      "19 Train total loss: -2.385151e+17 \tReconstruction loss: -4.533546e+17 \tLatent loss: 2.1483949e+17\n",
      "20 Train total loss: -3.1024597e+17 \tReconstruction loss: -5.895978e+17 \tLatent loss: 2.7935182e+17\n",
      "21 Train total loss: -3.971568e+17 \tReconstruction loss: -7.554767e+17 \tLatent loss: 3.583199e+17\n",
      "22 Train total loss: -5.0293517e+17 \tReconstruction loss: -9.566984e+17 \tLatent loss: 4.5376323e+17\n",
      "23 Train total loss: -6.282735e+17 \tReconstruction loss: -1.1953896e+18 \tLatent loss: 5.6711607e+17\n",
      "24 Train total loss: -7.7695e+17 \tReconstruction loss: -1.4788758e+18 \tLatent loss: 7.019258e+17\n",
      "25 Train total loss: -9.511996e+17 \tReconstruction loss: -1.8110743e+18 \tLatent loss: 8.598747e+17\n",
      "26 Train total loss: -1.1550324e+18 \tReconstruction loss: -2.1989778e+18 \tLatent loss: 1.0439453e+18\n",
      "27% Train total loss: -1.3901409e+18 \tReconstruction loss: -2.6470935e+18 \tLatent loss: 1.2569526e+18\n",
      "28 Train total loss: -1.6601237e+18 \tReconstruction loss: -3.1625278e+18 \tLatent loss: 1.502404e+18\n",
      "29 Train total loss: -1.9704477e+18 \tReconstruction loss: -3.7528044e+18 \tLatent loss: 1.7823567e+18\n",
      "30 Train total loss: -2.319156e+18 \tReconstruction loss: -4.4201975e+18 \tLatent loss: 2.1010417e+18\n",
      "31 Train total loss: -2.718979e+18 \tReconstruction loss: -5.179517e+18 \tLatent loss: 2.4605382e+18\n",
      "32 Train total loss: -3.1642548e+18 \tReconstruction loss: -6.031386e+18 \tLatent loss: 2.867131e+18\n",
      "33 Train total loss: -3.6642833e+18 \tReconstruction loss: -6.984657e+18 \tLatent loss: 3.3203737e+18\n",
      "34 Train total loss: -4.2225447e+18 \tReconstruction loss: -8.051125e+18 \tLatent loss: 3.8285803e+18\n",
      "35% Train total loss: -4.844844e+18 \tReconstruction loss: -9.237362e+18 \tLatent loss: 4.392518e+18\n",
      "36 Train total loss: -5.5320317e+18 \tReconstruction loss: -1.0552572e+19 \tLatent loss: 5.02054e+18\n",
      "37 Train total loss: -6.294517e+18 \tReconstruction loss: -1.2005386e+19 \tLatent loss: 5.710869e+18\n",
      "38 Train total loss: -7.137609e+18 \tReconstruction loss: -1.3613932e+19 \tLatent loss: 6.476323e+18\n",
      "39 Train total loss: -8.056212e+18 \tReconstruction loss: -1.5369445e+19 \tLatent loss: 7.3132334e+18\n",
      "40 Train total loss: -9.0643156e+18 \tReconstruction loss: -1.7298883e+19 \tLatent loss: 8.234567e+18\n",
      "41 Train total loss: -1.0166886e+19 \tReconstruction loss: -1.9400777e+19 \tLatent loss: 9.233891e+18\n",
      "42 Train total loss: -1.1367801e+19 \tReconstruction loss: -2.1697446e+19 \tLatent loss: 1.0329645e+19\n",
      "43 Train total loss: -1.2670897e+19 \tReconstruction loss: -2.4187026e+19 \tLatent loss: 1.1516129e+19\n",
      "44 Train total loss: -1.4086078e+19 \tReconstruction loss: -2.689449e+19 \tLatent loss: 1.2808412e+19\n",
      "45 Train total loss: -1.5634378e+19 \tReconstruction loss: -2.9837913e+19 \tLatent loss: 1.4203535e+19\n",
      "46 Train total loss: -1.7286841e+19 \tReconstruction loss: -3.3003583e+19 \tLatent loss: 1.5716742e+19\n",
      "47 Train total loss: -1.9065083e+19 \tReconstruction loss: -3.6403357e+19 \tLatent loss: 1.7338275e+19\n",
      "48 Train total loss: -2.0987018e+19 \tReconstruction loss: -4.0086056e+19 \tLatent loss: 1.9099038e+19\n",
      "49 Train total loss: -2.3056807e+19 \tReconstruction loss: -4.4032015e+19 \tLatent loss: 2.0975207e+19\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        #n_batches = mnist.train.num_examples // batch_size\n",
    "        n_batches = data_reader.num_batches\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = data_reader.next_batch()\n",
    "            #X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_val, reconstruction_loss_val, latent_loss_val = sess.run([loss, reconstruction_loss, latent_loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val, \"\\tReconstruction loss:\", reconstruction_loss_val, \"\\tLatent loss:\", latent_loss_val)\n",
    "        saver.save(sess, \"./my_model_variational.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
