{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Andrei provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rivas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f1686690942b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Agg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mplotting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotting'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of a Variational Fair Autoencoder. With no MMD just yet.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_reconstructed_digits(X, outputs, model_path = None, n_test_digits = 2):\n",
    "    with tf.Session() as sess:\n",
    "        if model_path:\n",
    "            saver.restore(sess, model_path)\n",
    "        X_test = mnist.test.images[:n_test_digits]\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_test})\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 3 * n_test_digits))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(X_test[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup logdir\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Loading MNIST data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construction phase\n",
    "n_s = 10\n",
    "n_inputs = 28*28 - n_s\n",
    "# encoders\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 20 # codings\n",
    "n_hidden3 = 500\n",
    "n_hidden4 = 20\n",
    "# decoders\n",
    "n_hidden5 = 500\n",
    "n_hidden6 = 20\n",
    "n_hidden7 = 500\n",
    "# n_hidden8 = 20\n",
    "# final output can take a random sample from the posterior\n",
    "n_outputs = n_inputs + n_s\n",
    "\n",
    "alpha = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        activation_fn = tf.nn.elu,\n",
    "        weights_initializer = tf.contrib.layers.variance_scaling_initializer()):\n",
    "    X = tf.placeholder(tf.float32, shape = [None, n_inputs], name=\"X_wo_s\")\n",
    "    s = tf.placeholder(tf.float32, shape = [None, n_s], name=\"s\")\n",
    "    X_full = tf.concat([X,s], axis=1)\n",
    "    y = tf.placeholder(tf.int32, shape = [None, 1], name=\"y\")\n",
    "    is_unlabelled = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "    with tf.name_scope(\"X_encoder\"):\n",
    "        hidden1 = fully_connected(tf.concat([X, s], axis=1), n_hidden1)\n",
    "        hidden2_mean = fully_connected(hidden1, n_hidden2, activation_fn = None)\n",
    "        hidden2_gamma = fully_connected(hidden1, n_hidden2, activation_fn = None)\n",
    "        hidden2_sigma = tf.exp(0.5 * hidden2_gamma)\n",
    "    noise1 = tf.random_normal(tf.shape(hidden2_sigma), dtype=tf.float32)\n",
    "    hidden2 = hidden2_mean + hidden2_sigma * noise1         # z1\n",
    "    with tf.name_scope(\"Z1_encoder\"):\n",
    "        hidden3_ygz1 = fully_connected(hidden2, n_hidden4, activation_fn = tf.nn.tanh)\n",
    "        hidden4_softmax_mean = fully_connected(hidden3_ygz1, 10, activation_fn = tf.nn.softmax)\n",
    "        if is_unlabelled == True:\n",
    "            # impute by sampling from q(y|z1)\n",
    "            y = tf.assign(y, tf.multinomial(hidden4_softmax_mean, 1,\n",
    "                                output_type = tf.int32))\n",
    "        hidden3 = fully_connected(tf.concat([hidden2, tf.cast(y, tf.float32)], axis=1),\n",
    "                        n_hidden3, activation_fn=tf.nn.tanh)\n",
    "        hidden4_mean = fully_connected(hidden3, n_hidden4, activation_fn = None)\n",
    "        hidden4_gamma = fully_connected(hidden3, n_hidden4, activation_fn = None)\n",
    "        hidden4_sigma = tf.exp(0.5 * hidden4_gamma)\n",
    "    noise2 = tf.random_normal(tf.shape(hidden4_sigma), dtype=tf.float32)\n",
    "    hidden4 = hidden4_mean + hidden4_sigma * noise2     # z2\n",
    "    with tf.name_scope(\"Z1_decoder\"):\n",
    "        hidden5 = fully_connected(tf.concat([hidden4, tf.cast(y, tf.float32)], axis=1 ),\n",
    "                    n_hidden5, activation_fn = tf.nn.tanh)\n",
    "        hidden6_mean = fully_connected(hidden5, n_hidden6, activation_fn = None)\n",
    "        hidden6_gamma = fully_connected(hidden5, n_hidden6, activation_fn = None)\n",
    "        hidden6_sigma = tf.exp(0.5 * hidden6_gamma)\n",
    "    noise3 = tf.random_normal(tf.shape(hidden6_sigma), dtype=tf.float32)\n",
    "    hidden6 = hidden6_mean + hidden6_sigma * noise3     # z1 (decoded)\n",
    "    with tf.name_scope(\"X_decoder\"):\n",
    "        hidden7 = fully_connected(tf.concat([hidden6, s], axis=1), n_hidden7,\n",
    "                                 activation_fn = tf.nn.tanh)\n",
    "        hidden8 = fully_connected(hidden7, n_outputs, activation_fn = None)\n",
    "    outputs = tf.sigmoid(hidden8, name=\"decoded_X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expected lower bound\n",
    "with tf.name_scope(\"ELB\"):\n",
    "    kl_z2 = 0.5 * tf.reduce_sum(\n",
    "                    tf.exp(hidden4_gamma)\n",
    "                    + tf.square(hidden4_mean)\n",
    "                    - 1\n",
    "                    - hidden4_gamma\n",
    "                    )\n",
    "\n",
    "    kl_z1 = 0.5 * (tf.reduce_sum(\n",
    "                    (1 / (1e-10 + tf.exp(hidden6_gamma))) * tf.exp(hidden2_gamma)\n",
    "                    - 1\n",
    "                    + hidden6_gamma\n",
    "                    - hidden2_gamma\n",
    "                    ) + tf.einsum('ij,ji -> i',\n",
    "                        (hidden6_mean-hidden2_mean) * (1 / (1e-10 + tf.exp(hidden6_gamma))),\n",
    "                        tf.transpose((hidden6_mean-hidden2_mean))))\n",
    "\n",
    "    indices = tf.range(tf.shape(y)[0])\n",
    "    indices = tf.concat([indices[:, tf.newaxis], y], axis=1)\n",
    "    eps = 1e-10\n",
    "    log_q_y_z1 = tf.reduce_sum(tf.log(eps + tf.gather_nd(hidden4_softmax_mean, indices)))\n",
    "\n",
    "    # Bernoulli log-likelihood\n",
    "    reconstruction_loss = -(tf.reduce_sum(X_full * tf.log(outputs)\n",
    "                            + (1 - X_full) * tf.log(1 - outputs)))\n",
    "    cost = kl_z2 + kl_z1 + reconstruction_loss + alpha * log_q_y_z1\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost)\n",
    "\n",
    "cost_summary = tf.summary.scalar('ELB', cost)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# Training\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_digits = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train total loss: 16227.459 \tReconstruction loss: 16963.402 \tKL-z1: 551.0965 \tKL-z2: 760.8822 \tlog_q(y|z1): -2047.922\n",
      "1 Train total loss: 14084.643 \tReconstruction loss: 14574.775 \tKL-z1: 736.3108 \tKL-z2: 866.88464 \tlog_q(y|z1): -2093.3286\n",
      "2 Train total loss: 14053.73 \tReconstruction loss: 14551.799 \tKL-z1: 639.9159 \tKL-z2: 841.39813 \tlog_q(y|z1): -1979.3823\n",
      "3 Train total loss: 13243.682 \tReconstruction loss: 13871.496 \tKL-z1: 518.6262 \tKL-z2: 879.3666 \tlog_q(y|z1): -2025.8073\n",
      "49% Train total loss: 13064.837 \tReconstruction loss: 13751.563 \tKL-z1: 527.91284 \tKL-z2: 857.4044 \tlog_q(y|z1): -2072.0437\n",
      "5 Train total loss: 12132.647 \tReconstruction loss: 12779.967 \tKL-z1: 507.21765 \tKL-z2: 825.4978 \tlog_q(y|z1): -1980.0354\n",
      "6 Train total loss: 11868.826 \tReconstruction loss: 12582.609 \tKL-z1: 486.0949 \tKL-z2: 849.28564 \tlog_q(y|z1): -2049.1643\n",
      "7 Train total loss: 12906.422 \tReconstruction loss: 13587.896 \tKL-z1: 485.3978 \tKL-z2: 905.3528 \tlog_q(y|z1): -2072.2256\n",
      "8 Train total loss: 12530.766 \tReconstruction loss: 13391.954 \tKL-z1: 492.67383 \tKL-z2: 879.5698 \tlog_q(y|z1): -2233.4312\n",
      "9 Train total loss: 12037.232 \tReconstruction loss: 12720.304 \tKL-z1: 483.93872 \tKL-z2: 882.2402 \tlog_q(y|z1): -2049.2495\n",
      "10 Train total loss: 11806.353 \tReconstruction loss: 12566.581 \tKL-z1: 448.1843 \tKL-z2: 909.9264 \tlog_q(y|z1): -2118.3386\n",
      "11 Train total loss: 11713.152 \tReconstruction loss: 12320.512 \tKL-z1: 424.76196 \tKL-z2: 925.0482 \tlog_q(y|z1): -1957.1697\n",
      "12 Train total loss: 12221.17 \tReconstruction loss: 12847.672 \tKL-z1: 466.32898 \tKL-z2: 887.3721 \tlog_q(y|z1): -1980.2032\n",
      "13 Train total loss: 12138.163 \tReconstruction loss: 12870.547 \tKL-z1: 488.247 \tKL-z2: 897.73096 \tlog_q(y|z1): -2118.362\n",
      "14 Train total loss: 11829.218 \tReconstruction loss: 12424.456 \tKL-z1: 538.27374 \tKL-z2: 915.7771 \tlog_q(y|z1): -2049.2888\n",
      "15 Train total loss: 10696.665 \tReconstruction loss: 11540.518 \tKL-z1: 410.1873 \tKL-z2: 933.40625 \tlog_q(y|z1): -2187.4463\n",
      "16 Train total loss: 11594.941 \tReconstruction loss: 12355.135 \tKL-z1: 506.5946 \tKL-z2: 920.661 \tlog_q(y|z1): -2187.4487\n",
      "17 Train total loss: 12872.865 \tReconstruction loss: 13490.794 \tKL-z1: 595.02356 \tKL-z2: 905.421 \tlog_q(y|z1): -2118.3726\n",
      "18 Train total loss: 11262.17 \tReconstruction loss: 11939.018 \tKL-z1: 492.54816 \tKL-z2: 902.927 \tlog_q(y|z1): -2072.3228\n",
      "19% Train total loss: 12183.491 \tReconstruction loss: 12926.836 \tKL-z1: 435.27417 \tKL-z2: 939.75635 \tlog_q(y|z1): -2118.375\n",
      "20 Train total loss: 11193.865 \tReconstruction loss: 11913.424 \tKL-z1: 378.59433 \tKL-z2: 951.1455 \tlog_q(y|z1): -2049.2983\n",
      "21 Train total loss: 11317.317 \tReconstruction loss: 11896.701 \tKL-z1: 467.54916 \tKL-z2: 933.2874 \tlog_q(y|z1): -1980.221\n",
      "22 Train total loss: 11596.871 \tReconstruction loss: 12248.236 \tKL-z1: 419.9914 \tKL-z2: 931.8912 \tlog_q(y|z1): -2003.248\n",
      "23% Train total loss: 11000.155 \tReconstruction loss: 11774.301 \tKL-z1: 403.1399 \tKL-z2: 941.0923 \tlog_q(y|z1): -2118.3777\n",
      "24 Train total loss: 11244.088 \tReconstruction loss: 11834.539 \tKL-z1: 526.3526 \tKL-z2: 955.5223 \tlog_q(y|z1): -2072.3262\n",
      "25 Train total loss: 10013.834 \tReconstruction loss: 10734.127 \tKL-z1: 447.79706 \tKL-z2: 927.26135 \tlog_q(y|z1): -2095.352\n",
      "26 Train total loss: 11720.86 \tReconstruction loss: 12360.753 \tKL-z1: 441.70734 \tKL-z2: 921.6493 \tlog_q(y|z1): -2003.2488\n",
      "27 Train total loss: 11853.082 \tReconstruction loss: 12571.508 \tKL-z1: 398.49075 \tKL-z2: 932.38403 \tlog_q(y|z1): -2049.3003\n",
      "28 Train total loss: 10930.849 \tReconstruction loss: 11620.268 \tKL-z1: 522.5759 \tKL-z2: 906.3827 \tlog_q(y|z1): -2118.378\n",
      "29 Train total loss: 11176.447 \tReconstruction loss: 11883.101 \tKL-z1: 416.99893 \tKL-z2: 948.67365 \tlog_q(y|z1): -2072.3264\n",
      "30 Train total loss: 10906.597 \tReconstruction loss: 11490.551 \tKL-z1: 434.06277 \tKL-z2: 893.1289 \tlog_q(y|z1): -1911.1455\n",
      "31 Train total loss: 11535.33 \tReconstruction loss: 12253.956 \tKL-z1: 428.6623 \tKL-z2: 925.03845 \tlog_q(y|z1): -2072.3267\n",
      "32 Train total loss: 11867.389 \tReconstruction loss: 12288.937 \tKL-z1: 470.59088 \tKL-z2: 949.9292 \tlog_q(y|z1): -1842.068\n",
      "33% Train total loss: 11040.746 \tReconstruction loss: 11810.487 \tKL-z1: 356.69257 \tKL-z2: 945.89215 \tlog_q(y|z1): -2072.3267\n",
      "34 Train total loss: 12575.033 \tReconstruction loss: 13151.443 \tKL-z1: 522.18085 \tKL-z2: 973.73535 \tlog_q(y|z1): -2072.3264\n",
      "35 Train total loss: 11074.701 \tReconstruction loss: 11799.314 \tKL-z1: 452.87234 \tKL-z2: 986.94366 \tlog_q(y|z1): -2164.4302\n",
      "36 Train total loss: 11478.518 \tReconstruction loss: 12183.54 \tKL-z1: 427.46158 \tKL-z2: 985.8935 \tlog_q(y|z1): -2118.3782\n",
      "37 Train total loss: 11330.542 \tReconstruction loss: 11891.25 \tKL-z1: 505.91187 \tKL-z2: 959.65405 \tlog_q(y|z1): -2026.2748\n",
      "38 Train total loss: 10817.735 \tReconstruction loss: 11588.706 \tKL-z1: 392.17334 \tKL-z2: 955.234 \tlog_q(y|z1): -2118.3782\n",
      "39 Train total loss: 10610.648 \tReconstruction loss: 11308.593 \tKL-z1: 407.15466 \tKL-z2: 967.22705 \tlog_q(y|z1): -2072.3264\n",
      "40 Train total loss: 11858.881 \tReconstruction loss: 12512.957 \tKL-z1: 390.20752 \tKL-z2: 935.93915 \tlog_q(y|z1): -1980.2231\n",
      "41 Train total loss: 11140.576 \tReconstruction loss: 11793.215 \tKL-z1: 434.66272 \tKL-z2: 938.97314 \tlog_q(y|z1): -2026.2749\n",
      "42% Train total loss: 11086.141 \tReconstruction loss: 11837.092 \tKL-z1: 407.64154 \tKL-z2: 913.73334 \tlog_q(y|z1): -2072.3264\n",
      "43 Train total loss: 11168.215 \tReconstruction loss: 11897.37 \tKL-z1: 371.20892 \tKL-z2: 971.96204 \tlog_q(y|z1): -2072.3264\n",
      "44 Train total loss: 11199.331 \tReconstruction loss: 12055.05 \tKL-z1: 401.47614 \tKL-z2: 976.31274 \tlog_q(y|z1): -2233.5076\n",
      "45 Train total loss: 10651.731 \tReconstruction loss: 11304.193 \tKL-z1: 433.42172 \tKL-z2: 940.3906 \tlog_q(y|z1): -2026.2748\n",
      "46 Train total loss: 11535.072 \tReconstruction loss: 12262.995 \tKL-z1: 420.46515 \tKL-z2: 946.96436 \tlog_q(y|z1): -2095.3525\n",
      "47 Train total loss: 10961.798 \tReconstruction loss: 11751.02 \tKL-z1: 430.68152 \tKL-z2: 921.50104 \tlog_q(y|z1): -2141.404\n",
      "48 Train total loss: 11051.288 \tReconstruction loss: 11825.328 \tKL-z1: 431.0343 \tKL-z2: 982.3818 \tlog_q(y|z1): -2187.4558\n",
      "49% Train total loss: 11662.673 \tReconstruction loss: 12399.927 \tKL-z1: 387.833 \tKL-z2: 970.266 \tlog_q(y|z1): -2095.3525\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch[:,:-n_s],\n",
    "                                    s: X_batch[:,-n_s:],\n",
    "                                    y: y_batch[:,np.newaxis],\n",
    "                                    is_unlabelled: False})\n",
    "        kl_z2_val, kl_z1_val, log_q_y_z1_val, reconstruction_loss_val, loss_val = sess.run([\n",
    "                kl_z2,\n",
    "                kl_z1,\n",
    "                log_q_y_z1,\n",
    "                reconstruction_loss,\n",
    "                cost],\n",
    "                feed_dict={X: X_batch[:,:-n_s],\n",
    "                        s: X_batch[:,-n_s:],\n",
    "                        y: y_batch[:,np.newaxis]})\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val,\n",
    "         \"\\tReconstruction loss:\", reconstruction_loss_val,\n",
    "          \"\\tKL-z1:\", kl_z1_val,\n",
    "          \"\\tKL-z2:\", kl_z2_val,\n",
    "          \"\\tlog_q(y|z1):\", log_q_y_z1_val)\n",
    "        # saver.save(sess, \"./my_model_all_layers.ckpt\")\n",
    "\n",
    "    # generating digits\n",
    "    codings_rnd = np.random.normal(scale=2,size=[n_digits, n_hidden2])\n",
    "    s_rnd = X_batch[:n_digits, -n_s:]\n",
    "    # s_rnd = np.random.choice(2, size=[n_digits, n_s], p = [0.98, 0.02])\n",
    "    # codings_rnd = X_batch[:n_digits, :-n_s]\n",
    "    outputs_val = outputs.eval(feed_dict={hidden2: codings_rnd,\n",
    "                                        s: s_rnd,\n",
    "                                        y: np.zeros((n_digits,1), dtype=np.int32),\n",
    "                                        is_unlabelled: True})\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_multiple_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f6f23c53ac77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplot_multiple_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msave_fig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vfae_generated_digits_plot_s{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_multiple_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting the generated digits\n",
    "n_rows = 6\n",
    "n_cols = 10\n",
    "plot_multiple_images(outputs_val.reshape(-1, 28, 28), n_rows, n_cols)\n",
    "save_fig(\"vfae_generated_digits_plot_s{}\".format(n_s))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
